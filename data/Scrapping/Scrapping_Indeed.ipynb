{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> <center> Projet Scraper indeed </center> </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "browser = webdriver.Chrome()\n",
    "browser.get('https://www.indeed.fr/')\n",
    "browser.maximize_window()\n",
    "from time import sleep\n",
    "import re"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Faire un script de scraping sur indeed (https://www.indeed.fr/)  qui permette à l’user de spécifier le type d’annonces qu’il souhaite récupérer :\n",
    "-\tMétier (développeur, data scientist…)\n",
    "-\tType de contrat recherché (CDI, CDD, freelance…)\n",
    "-\tLieu de recherche (Paris, Toulouse, …)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Metier(string):\n",
    "    recherche = browser.find_element_by_xpath('//*[@id=\"text-input-what\"]')\n",
    "    recherche.click()\n",
    "    recherche.clear()\n",
    "    recherche.send_keys(string)\n",
    "    sleep(3)\n",
    "    recherche.send_keys(Keys.ENTER)\n",
    "\n",
    "def Localisation(string):\n",
    "    recherche = browser.find_element_by_xpath('//*[@id=\"where\"]')\n",
    "    recherche.click()\n",
    "    recherche.clear()\n",
    "    recherche.send_keys(string)\n",
    "    sleep(3)\n",
    "    recherche.send_keys(Keys.ENTER)\n",
    "    \n",
    "def Type_Contrat(string):\n",
    "    if string.lower() == 'cdi':      \n",
    "        browser.find_element_by_xpath(\"//*[@id='JOB_TYPE_rbo']/ul/li[1]/a/span[1]\").click()\n",
    "    if string.lower() == 'temps plein':\n",
    "        browser.find_element_by_xpath(\"//*[@id='JOB_TYPE_rbo']/ul/li[2]/a/span[1]\").click()\n",
    "    if string.lower() == 'stage':\n",
    "        browser.find_element_by_xpath(\"//*[@id='JOB_TYPE_rbo']/ul/li[3]/a/span[1]\").click()\n",
    "    if string.lower() == 'cdd':\n",
    "        browser.find_element_by_xpath(\"//*[@id='JOB_TYPE_rbo']/ul/li[4]/a/span[1]\").click()\n",
    "    if string.lower() == 'apprentissage':\n",
    "        browser.find_element_by_xpath(\"//*[@id='JOB_TYPE_rbo']/ul/li[5]/a/span[1]\").click()\n",
    "    if string.lower() == 'contrat pro':\n",
    "        browser.find_element_by_xpath(\"//*[@id='JOB_TYPE_rbo']/ul/li[6]/a/span[1]\").click()\n",
    "    if string.lower() == 'freelance / indépendant':\n",
    "        browser.find_element_by_xpath(\"//*[@id='JOB_TYPE_rbo']/ul/li[7]/a/span[1]\").click()\n",
    "    if string.lower() == 'intérim':\n",
    "        browser.find_element_by_xpath(\"//*[@id='JOB_TYPE_rbo']/ul/li[8]/a/span[1]\").click()\n",
    "    if string.lower() == 'temps partiel':\n",
    "        browser.find_element_by_xpath(\"//*[@id='JOB_TYPE_rbo']/ul/li[9]/a/span[1]\").click()    "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Les infos à scraper :\n",
    "-\tTitre\n",
    "-\tNom de la boite\n",
    "-\tAdresse\n",
    "-\tSalaire\n",
    "-\tDescriptif du poste\n",
    "-\tDate de publication de l’annonce\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Liste  à remplir**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Metier(\"data scientist\")\n",
    "Localisation(\"Paris\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Type_Contrat('cdi')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creation des listes vides pour chaque colonne**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "Titre = []\n",
    "Nom_Entreprise = []\n",
    "Adresse = []\n",
    "Salaire = []\n",
    "Descriptif_du_poste = []\n",
    "Date_de_publication = []\n",
    "\n",
    "\n",
    "while True:\n",
    "    annonces = browser.find_elements_by_class_name(\"jobsearch-SerpJobCard\")\n",
    "    for i in range(0,len(annonces)):\n",
    "        try:\n",
    "            titre = annonces[i].find_element_by_class_name('title')\n",
    "            Titre.append(titre.text)\n",
    "        except:\n",
    "            Titre.append('vide')\n",
    "        try:\n",
    "            nom = annonces[i].find_element_by_class_name('company')\n",
    "            Nom_Entreprise.append(nom.text)\n",
    "\n",
    "        except:\n",
    "            Nom_Entreprise.append('vide')\n",
    "        try:\n",
    "            adresse= annonces[i].find_element_by_class_name('location')\n",
    "            Adresse.append(adresse.text)\n",
    "        except:\n",
    "            Adresse.append('vide')\n",
    "        try:\n",
    "            date = annonces[i].find_element_by_class_name('date')\n",
    "            Date_de_publication.append(date.text)\n",
    "\n",
    "        except:\n",
    "            Date_de_publication.append('vide')\n",
    "        try:\n",
    "            salaire = annonces[i].find_element_by_class_name('salaryText')\n",
    "            Salaire.append(salaire.text)\n",
    "        except:\n",
    "            Salaire.append('vide')\n",
    "        try:\n",
    "            annonces[i].find_element_by_class_name('title').click()\n",
    "            sleep(2)\n",
    "            descriptif= browser.find_element_by_xpath(\"//*[@id='vjs-content']\")\n",
    "            Descriptif_du_poste.append(descriptif.text)\n",
    "        except:\n",
    "            Descriptif_du_poste.append('vide')\n",
    "\n",
    "    \n",
    "    try:\n",
    "        browser.find_element_by_xpath(\"//*[contains(text(), 'Suivant')]\").click() \n",
    "        sleep(2)\n",
    "    except:\n",
    "        break\n",
    "\n",
    "df = pd.DataFrame({\"Titre\":Titre,\"Nom_Entreprise\":Nom_Entreprise,\"Adresse\":Adresse,\n",
    "                  \"Salaire\":Salaire,\"Descriptif_du_poste\":Descriptif_du_poste,\"Date_de_publication\":Date_de_publication}) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scrapping et remplissage des listes utiles pour le remplissage du df par la suite**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remplissage du dataframe et nettoyage**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Enregistrement des données dans BDD mongo**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Brouillon avec beautifulsoup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL= \"https://www.indeed.fr/jobs?q=data+scientist&l=paris\"\n",
    "page = requests.get(URL)\n",
    "soup = BeautifulSoup(page.text, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# titre du job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['INTERN - Data Scientist (m/w/d)',\n",
       " 'Senior Backend Developer (f/m/x) (relocated to Germany)',\n",
       " 'Data Scientist – Stagiaire H/F',\n",
       " 'Data Scientist Assistant - H/F (stage)',\n",
       " 'DATA SCIENTIST INTERN - MACHINE LEARNING - AI',\n",
       " 'Data scientist Junior H/F',\n",
       " 'CDI Data Analyst - Scientist - F/H',\n",
       " 'STAGE - Assistant(e) Data Scientist CRM -EMEA',\n",
       " 'Data scientist-(H/F)',\n",
       " 'DATA SCIENTIST - H/F',\n",
       " 'Stage - Assistant data scientist-(H/F)',\n",
       " 'Data Scientist - Paris']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_job_title_from_result(soup): \n",
    "    jobs = []\n",
    "    for div in soup.find_all(name=\"div\", attrs={\"class\":\"row\"}):\n",
    "        for a in div.find_all(name=\"a\", attrs={\"data-tn-element\":\"jobTitle\"}):\n",
    "            jobs.append(a[\"title\"])\n",
    "    return(jobs)\n",
    "titre = extract_job_title_from_result(soup)\n",
    "titre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(titre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compagny Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_company_from_result(soup): \n",
    "    companies = []\n",
    "    for div in soup.find_all(name=\"div\", attrs={\"class\":\"row\"}):\n",
    "        company = div.find_all(name=\"span\", attrs={\"class\":\"company\"})\n",
    "        if len(company) > 0:\n",
    "            for b in company:\n",
    "                companies.append(b.text.strip())\n",
    "        else:\n",
    "            sec_try = div.find_all(name=\"span\", attrs={\"class\":\"result-link-source\"})\n",
    "            for span in sec_try:\n",
    "                companies.append(span.text.strip())\n",
    "    return(companies)\n",
    " \n",
    "compagny = extract_company_from_result(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Teradata',\n",
       " 'Ubisoft',\n",
       " 'Ubisoft',\n",
       " 'Sensego',\n",
       " 'Vallourec',\n",
       " 'Groupe CANAL+',\n",
       " 'Société Générale',\n",
       " 'La Banque de France',\n",
       " 'Ubisoft',\n",
       " 'Fieldbox.ai',\n",
       " 'Ingeniance']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compagny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(compagny)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constat : le scrapping avec beautifulsoup ne correspondent pas exactement aux elements de la page \n",
    "# pourquoi ? \n",
    "# utiliser selenium "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Brouillon selenium**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Titre )\n",
    "print(Nom_Entreprise )\n",
    "print(Adresse )\n",
    "print(Salaire )\n",
    "print(Descriptif_du_poste )\n",
    "print(Date_de_publication )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    \n",
    "    titres = browser.find_elements_by_class_name('title')\n",
    "    for titre in titres: \n",
    "        Titre.append(titre.text)\n",
    "\n",
    "    noms = browser.find_elements_by_class_name('company')\n",
    "    for nom in noms: \n",
    "        Nom_Entreprise.append(nom.text)\n",
    "        \n",
    "    adresses = browser.find_elements_by_class_name('location')\n",
    "    for adresse in adresses: \n",
    "        Adresse.append(adresse.text)   \n",
    "    \n",
    "    dates = browser.find_elements_by_class_name('date')\n",
    "    for date in dates:\n",
    "        Date_de_publication.append(date.text)\n",
    "        \n",
    "    salaires = browser.find_elements_by_class_name('salary')\n",
    "    for salaire in salaires:\n",
    "        Salaire.append(salaire.text)\n",
    "        \n",
    "    descriptifs = browser.find_elements_by_class_name('summary')\n",
    "    for descriptif in descriptifs:\n",
    "        Descriptif_du_poste.append(descriptif.text)\n",
    "    \n",
    "    try:\n",
    "        browser.find_element_by_class_name('pn').click() # \n",
    "        sleep(5)\n",
    "    except:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
